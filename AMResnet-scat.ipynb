{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa as lb\n",
    "from torch import nn\n",
    "from argparse import Namespace\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kymatio.numpy import Scattering1D\n",
    "from torch import from_numpy\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassPrecision\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from lightning.pytorch import seed_everything\n",
    "seed_everything(42)\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d788d09",
   "metadata": {},
   "source": [
    "## Data segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5112fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(data_path, audfile_dir, dest_fs, test_files=[], seg_time=0.09, hog_time=0.05):\n",
    "    data = pd.read_csv(data_path)\n",
    "    files = data['New_Filename'].values\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for name in files:\n",
    "        aud, fs = lb.load(os.path.join(audfile_dir, name), sr=dest_fs)\n",
    "        lbl = np.int16(data[data['New_Filename'] == name]['label'].values[0])\n",
    "        seg_size = int(seg_time * fs);\n",
    "        hog_size = int(hog_time * fs);\n",
    "        if name in test_files:\n",
    "            for i in range(0, len(aud)-hog_size, hog_size):\n",
    "                if len(aud) >= i+seg_size:\n",
    "                    X_test.append(aud[i:i+seg_size])\n",
    "                    Y_test.append(lbl)\n",
    "        else:\n",
    "            for i in range(0, len(aud)-hog_size, hog_size):\n",
    "                if len(aud) >= i+seg_size:\n",
    "                    X_train.append(aud[i:i+seg_size])\n",
    "                    Y_train.append(lbl)\n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5f4d2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shipseardataset(Dataset):\n",
    "    def __init__(self, data_path, test_files=[], mode ='train', val_tr_pat=0.15, \n",
    "                 test_part=0.9, sample_rate=51200, win_time=0.1, step_time=0.05, \n",
    "                 feature_param={'J':6, 'Q':8}):\n",
    "        audfile_dir = os.path.dirname(data_path)        \n",
    "        X_train, Y_train, X_test, Y_test = segment(data_path, audfile_dir, sample_rate, test_files, win_time, step_time)\n",
    "        if len(test_files)==0:\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=test_part,random_state=6)\n",
    "            \n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=val_tr_pat,random_state=6)\n",
    "        N = X_train.shape[1]\n",
    "        self.scattering = Scattering1D(feature_param['J'], N, feature_param['Q'])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.x = X_train\n",
    "            self.y = Y_train\n",
    "        elif mode == 'val':\n",
    "            self.x = X_val\n",
    "            self.y = Y_val\n",
    "        else:\n",
    "            self.x = X_test\n",
    "            self.y = Y_test\n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i] # BN X N\n",
    "        x = self.scattering(x)[np.newaxis]\n",
    "        #x = lb.feature.melspectrogram(y=x,hop_length=501)\n",
    "        #x = lb.power_to_db(x,ref=np.max)[np.newaxis]  # 1 X P X N/(2**J), P=1+JQ+J(J-1)Q/2.\n",
    "        y = self.y[i] # BN\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166530a",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77909e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##---------- Spatial Attention ----------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0,dilation=1, groups=1, relu=True, bn=True, bias=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class spatial_attn_layer(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super(spatial_attn_layer, self).__init__()\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1)//2, relu=False)\n",
    "    def forward(self, x):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## ------ Channel Attention --------------\n",
    "class ca_layer(nn.Module):\n",
    "    def __init__(self, channel, reduction=8, bias=True):\n",
    "        super(ca_layer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #这里定义了残差块内连续的2个卷积层\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            #shortcut，这里为了跟2个卷积层的结果结构一致，要做处理\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "        self.ca = ca_layer(outchannel)\n",
    "        self.sa = spatial_attn_layer()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = self.ca(out)\n",
    "        out = self.sa(out)\n",
    "        #将2个卷积层的输出跟处理过的x相加，实现ResNet的基本结构\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, ResBlock, in_channel=1, out_class=5):\n",
    "        super(network, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResBlock, 512, 2, stride=2)   \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.avg = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512, out_class)\n",
    "    #这个函数主要是用来，重复同一个残差块    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #在这里，整个ResNet18的结构就很清晰了\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.avg(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2026cf",
   "metadata": {},
   "source": [
    "## PLmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0afdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        self.net = network(ResBlock,in_channel=1,out_class=5)\n",
    "        print(self.net)        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()  # DiceLoss()\n",
    "        self.accuracy = MulticlassAccuracy(num_classes=5)\n",
    "        self.f1_score = MulticlassF1Score(num_classes=5)\n",
    "        self.precious = MulticlassPrecision(num_classes=5)\n",
    "        self.accuracy1 = MulticlassAccuracy(num_classes=5,average='none')\n",
    "        self.f1_score1 = MulticlassF1Score(num_classes=5,average='none')\n",
    "        self.precious1 = MulticlassPrecision(num_classes=5,average='none')\n",
    "        self.confuse = MulticlassConfusionMatrix(num_classes=5)\n",
    "        self.val_acc = 0\n",
    "        self.global_confuse = torch.zeros(5,5,device='cuda:0')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # called with self(x)\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x = batch[0].float() # NB X N\n",
    "        y = batch[1] # NB\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y.long())\n",
    "        self.accuracy.update(torch.argmax(y_hat,dim=1), y)\n",
    "        metric = self.accuracy.compute()        \n",
    "        # loss_all = loss_Dice\n",
    "        self.log(\"train/acc\", metric, on_step=False, on_epoch=True)\n",
    "        self.log(\"train/total_loss\", loss)\n",
    "        self.accuracy.reset()\n",
    "        \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x = batch[0].float() # NB X N\n",
    "        y = batch[1] # NB\n",
    "        y_hat = self(x)\n",
    "        self.accuracy.update(torch.argmax(y_hat,dim=1), y)\n",
    "        metric = self.accuracy.compute()\n",
    "        self.log(\"val_acc\", metric, on_step=False, on_epoch=True)\n",
    "        self.accuracy.reset()\n",
    "        \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x = batch[0].float() # NB X N\n",
    "        y = batch[1] # NB\n",
    "        y_hat = self(x)\n",
    "\n",
    "        self.accuracy.update(torch.argmax(y_hat,dim=1), y)\n",
    "        metric = self.accuracy.compute()\n",
    "        self.log(\"test_acc\", metric, on_step=False, on_epoch=True)\n",
    "        self.accuracy.reset()\n",
    "\n",
    "        self.f1_score.update(torch.argmax(y_hat,dim=1), y)\n",
    "        f1= self.f1_score.compute()\n",
    "        self.log(\"f1_score\", f1, on_step=False, on_epoch=True)\n",
    "        self.f1_score.reset()\n",
    "\n",
    "        self.precious.update(torch.argmax(y_hat,dim=1), y)\n",
    "        precious= self.precious.compute()\n",
    "        self.log(\"precious\", precious, on_step=False, on_epoch=True)\n",
    "        self.precious.reset()\n",
    "\n",
    "        self.accuracy1.update(torch.argmax(y_hat,dim=1), y)\n",
    "        metric = self.accuracy1.compute()  \n",
    "        for i,acc3 in enumerate(metric):      \n",
    "            self.log(f\"test_acc-{i}\", acc3, on_step=False, on_epoch=True)\n",
    "        self.accuracy1.reset()\n",
    "\n",
    "        self.f1_score1.update(torch.argmax(y_hat,dim=1), y)\n",
    "        f12= self.f1_score1.compute() \n",
    "        for i,acc4 in enumerate(f12):       \n",
    "            self.log(f\"f1_score-{i}\", acc4, on_step=False, on_epoch=True)\n",
    "        self.f1_score1.reset()\n",
    "\n",
    "        self.precious1.update(torch.argmax(y_hat,dim=1), y)\n",
    "        precious2= self.precious1.compute() \n",
    "        for i,acc5 in enumerate(precious2):       \n",
    "            self.log(f\"precious-{i}\", acc5, on_step=False, on_epoch=True)\n",
    "        self.precious1.reset()\n",
    "\n",
    "        self.confuse.update(torch.argmax(y_hat,dim=1), y)\n",
    "        confuse = self.confuse.compute()\n",
    "        self.global_confuse += confuse\n",
    "        print(self.global_confuse)\n",
    "        self.confuse.reset()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x = batch[0].float() # NB X N\n",
    "        y_hat = self(x)\n",
    "        res = torch.argmax(y_hat, dim=1)\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "#         optimizer = torch.optim.SGD(self.net.parameters(), lr=self.hparams.lr, momentum=0.9)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.hparams.lr,\n",
    "                                     betas=(self.hparams.b1, self.hparams.b2))\n",
    "        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.2, threshold=1e-4, patience=3, eps=0.001),\n",
    "            'interval': 'epoch',\n",
    "                        'monitor': 'val_acc'}\n",
    "#         lr_scheduler = {'scheduler': torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=0.01),\n",
    "#                         'interval': 'batch'}\n",
    "#         lr_scheduler = {'scheduler': torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5),\n",
    "#                         'interval': 'epoch'}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        train_dataset = shipseardataset(self.hparams.data_path, self.hparams.test_files, 'train', \n",
    "                                        val_tr_pat=self.hparams.var_par, test_part=self.hparams.test_par, \n",
    "                                        win_time=self.hparams.win_time, sample_rate=self.hparams.sample_rate, \n",
    "                                        step_time=self.hparams.step_time, \n",
    "                                        feature_param=self.hparams.feature_param)\n",
    "        return DataLoader(train_dataset, shuffle=True, pin_memory=True,\n",
    "                          batch_size=self.hparams.batch_size, num_workers=int(self.hparams.num_threads))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        val_dataset = shipseardataset(self.hparams.data_path, self.hparams.test_files, 'val', \n",
    "                                        val_tr_pat=self.hparams.var_par, test_part=self.hparams.test_par, \n",
    "                                        win_time=self.hparams.win_time, sample_rate=self.hparams.sample_rate, \n",
    "                                        step_time=self.hparams.step_time, \n",
    "                                        feature_param=self.hparams.feature_param)\n",
    "        return DataLoader(val_dataset, shuffle=False, pin_memory=True,\n",
    "                          batch_size=self.hparams.batch_size, num_workers=int(self.hparams.num_threads))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        test_data = shipseardataset(self.hparams.data_path, self.hparams.test_files, 'test', \n",
    "                                        val_tr_pat=self.hparams.var_par, test_part=self.hparams.test_par, \n",
    "                                        win_time=self.hparams.win_time, sample_rate=self.hparams.sample_rate, \n",
    "                                        step_time=self.hparams.step_time, \n",
    "                                        feature_param=self.hparams.feature_param)\n",
    "        return DataLoader(test_data, shuffle=False, pin_memory=True,\n",
    "                          batch_size=64, num_workers=int(self.hparams.num_threads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e75f9c",
   "metadata": {},
   "source": [
    "## Super-Paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8945199",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= {\n",
    "    'feature_param':{'J':6, 'Q':8},\n",
    "    'data_path': '/home/ubuntu/KFT/Data/underwater/shipsEar_AUDIOS/shipsEar.csv',\n",
    "    'var_par': 0.15,\n",
    "    'test_par': 0.9,\n",
    "    'sample_rate': 51200,\n",
    "    'win_time': 0.5,\n",
    "    'step_time': 0.3,\n",
    "    'in_channel':1,\n",
    "    'out_class':5,\n",
    "    'results_dir':'/home/ubuntu/KFT/Data/underwater/shipsEar_AUDIOS/result',\n",
    "    'results_name': 'model-0.9-scat',\n",
    "    'version':2,\n",
    "    'port':8099,\n",
    "    'batch_size': 32,\n",
    "    'num_threads': 4,\n",
    "    'lr': 0.001,\n",
    "    'b1': 0.9,\n",
    "    'b2': 0.999,\n",
    "    'max_epochs': 200,\n",
    "    'test_files': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01971bef",
   "metadata": {},
   "source": [
    "## PLTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881052c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyparams = Namespace(**parameters)\n",
    "\n",
    "plmodel = Model(hyparams)\n",
    "\n",
    "earlystop = pl.callbacks.EarlyStopping('val_acc', mode='max', min_delta=1e-4, patience=8)\n",
    "tb_logger = pl.loggers.TensorBoardLogger(hyparams.results_dir, hyparams.results_name, version=hyparams.version)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(tb_logger.log_dir, monitor='val_acc', mode='max',\n",
    "                                                  verbose=True)\n",
    "# lr_logger = pl.callbacks.lr_logger.LearningRateLogger()\n",
    "cmd = 'tensorboard --logdir %s --port %s' % (tb_logger.log_dir, hyparams.port)\n",
    "print(cmd)\n",
    "\n",
    "gpus = [0]\n",
    "tb_logger.log_hyperparams(hyparams)\n",
    "trainer = pl.Trainer( accelerator ='gpu', devices = 1,\n",
    "                     logger=tb_logger, max_epochs=hyparams.max_epochs, min_epochs=10,\n",
    "                     callbacks=[checkpoint_callback, earlystop])\n",
    "\n",
    "# checkpoint_dir = '/nas/home/fkong/data/farimage/simulation/fracture_new/preds/frac_multi/UNET/version_3/'\n",
    "# ckpt_name = [x for x in os.listdir(checkpoint_dir) if '.ckpt' in x][0]\n",
    "\n",
    "# segmodel = UnetModel.load_from_checkpoint(\n",
    "#     os.path.join(checkpoint_dir, ckpt_name),\n",
    "#     map_location=None)\n",
    "# segmodel.current_epoch = 0\n",
    "now = time.time()\n",
    "trainer.fit(plmodel)\n",
    "finish = time.time()\n",
    "print(finish-now)\n",
    "# trainer.test(segmodel)\n",
    "\n",
    "# checkpoint_dir = os.path.join(tb_logger.log_dir)\n",
    "# ckpt_name = [x for x in os.listdir(checkpoint_dir) if '.ckpt' in x][0]\n",
    "\n",
    "# segmodel = UnetModel.load_from_checkpoint(\n",
    "#     os.path.join(checkpoint_dir, ckpt_name),\n",
    "#     map_location=None, hparams=hyparams)\n",
    "\n",
    "trainer.test(plmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f07c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = shipseardataset(hyparams.data_path, hyparams.test_files, 'test')\n",
    "# test_loader = DataLoader(test_data)\n",
    "\n",
    "# trainer.test(plmodel,dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32158a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/ubuntu/KFT/Data/underwater/shipsEar_AUDIOS/result/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
